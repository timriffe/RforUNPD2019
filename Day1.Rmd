---
pagetitle: "Day 1"
title: | 
  | Introduction to R for Demographers
  | \vspace{1.5cm} \LARGE\emph{Workshop outline}
author: |
  | United Nations Population Division
  | 19-23 August, 2019
  | Tim Riffe
  | Max-Planck-Institute for Demographic Research
date: "19 August, 2019"
output:
  html_document:
    number_sections: yes
    toc: yes
params:
  output_dir: "../RforUNPD2019/docs"
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center}\includegraphics[trim=0 0 0 8cm, width=6cm, ]{assets/MPIDR_square_color.pdf}\\[\bigskipamount]}
- \posttitle{\end{center}}
bibliography: bibliography.bib
---

<a href="https://github.com/timriffe/RforUNPD2019" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About me
				
Hi, I'm Tim Riffe, a US-American, [CED](https://ced.uab.cat/)-educated (Spain) demographer living in Germany and working at the [MPIDR](https://www.demogr.mpg.de/en/). I've been hooked on `R` since 2009 when I did [EDSD](https://www.eds-demography.org/), and have since authored several packages, mostly that do demographic things, but sometimes that do plotting things, and I've even written a [package](https://github.com/timriffe/DemoTools) or [two](https://github.com/timriffe/DDSQLtools) for you ([with help](https://github.com/timriffe/DemoTools/graphs/contributors) of course). My own research data preparation, analyses, analytic plotting, and presentation-quality plotting are all done in `R`. I've led [workshops](https://github.com/timriffe/BSSD2019Module1) and [courses](https://github.com/timriffe/DemoED) such as this one, but yours will be unique and more ambitious.
				
# About this workshop

In the past `R` introductions such as this were done exclusively using the tools of so-called base-`R`, with the objective of putting new users on the path to programming. Nowadays it is becoming more common to teach intro to `R` from a data analyst perspective, namely to use the already-implemented tools and frameworks of `R` to just get things done efficiently without really needing to get into the nitty gritty of programming. Such courses tend to use the [tidyverse](https://www.tidyverse.org/) framework 

Your route to being an `R` data analyst/visualizer could be via Base programming, as it was for me, but it is a longer road than simply jumping into the `tidyverse`. I will try to give you introduction to both things.

So this is what we're going to do: Today we'll go through some very basic basics and then dip into the tidyverse approach, and then in following sessions we'll shift gears to base-`R` programming in order to introduce some concepts that will augment your abilities and ultimately your experience of `R`. These will most importantly include function-writing, loops, and conditional execution. New concepts will be introduced in an interactive type-along way. After a few examples of each new concept, I'll always try to loop back to the *tidy* approach, slipping in new tidy concepts. 

This is an absolute intro course, I don't expect you'll have any experience with `R` or have even heard of these concepts. I do expect that you're familiar with other statistical packages or programming languages at an advanced level, so I'll try to use analogies where possible. I expect you'll be able to keep up if you type along and take notes as we go, and if you ask questions and tell me to stop, slow down, or repeat when needed. Real learning will happen when you try to apply these concepts on your own, and for this reason I'll give afternoon exercises, which we'll then solve together. Often I will set myself up for apparent failure by not having worked through a solution in advance, and you'll see me induce errors and use help resources in a natural way. It's important to see this messy side of programming so that you also learn to make natural use of such resources.

# `R` and the `Rstudio` environment

`R` is a language. You should download and install the latest version from here:
[https://cran.r-project.org/](https://cran.r-project.org/) -- pick the one that runs on your operating system.

`R` is simple and lightweight. It's possible to work directly in its console, or even from the terminal, but we'll instead use a very user-friendly application to interract with it:

`Rstudio` is an application. You should download and install the latest version from here:
[https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/) -- again, look at the OS specs and pick the one that pairs well with your operating system.

`Rstudio` is many things at once. It is a file manager, a script editor, a document editor, an instance of `R`, a data viewer, even a web browser. This is where we'll do everything. We will be working often in the form of `R`*markdown* documents, which allows us to mix code, output, and text in a single file. This very document was written in `R` markdown, and it can generate nice-looking documents in Word, html, or pdf (I'm not exactly taking advantage of all its bells and whistles). We'll get started right away with this. The other way we'll write code is in `R` scripts, which are just text files with the `.R` or `.r` file extension.

*** Brief intermission for Rstudio tour and demonstration ***

TO-DO list for the demonstration:
1. Rstudio panes and tabs
2. arithmetic in the console
3. start a `.project` for this workshop
4. make a folder for Day 1, Monday
5. make a new `R` markdown file for this session
6. show how to use `R` from markdown
7. knit
8. yell hurrah

Much of the `R` interaction we do in this first week of the workshop will be *from* Rmarkdown. This will become second nature, and you can take that with you. I'll comment other Rstudio features as we go.

# `R` basics

## `R` sessions
We often speak of `R` sessions, a given interactive instance of `R`. For the most part `R` sessions are in-memory. If you have a dataset (or many) loaded into a session, then these are stored as *objects* in your workspace or *environment*. You can have a clean or a cluttery environment. You can take a look at what's in your environment in the `environment` tab of Rstudio (upper right corner by default). When you close an instance of `R` you can save your environment as-is, or you can lose it. I recommend *not* saving it. Otherwise you'll end up with lots of file baggage. It's more efficient and less bug-prone to save code and data in files than to save a bunch of saved `R` sessions. You can wipe your session free in Rstudio by selecting `Restart R` under `session` tab, or `Ctrl + Shift + F10`, or by executing `rstudioapi::restartSession()` in the console.

## `R` is functional
`R` is a *functional* programming language. You can think of functions in different ways-- they're like the way functions are used in mathematics, $f()$, or you could think of them as little programs. You give the function input (data, instructions), and then it does something with that and returns output.

Here's a cheap example:
```{r}
sum(1,2,3,4)
```
*sum* is just the name of the function instead of $f()$, and just the same, we enclose its inputs in round parentheses. `R` has many many such functions available to you. They can be combined in many many ways, and you can write your own functions too. If you get the hang of it, you'll find that functions can help you structure thought and analyses. We'll learn a lot about them later this week, but we'll start using them today without thinking about it much. We call `R` a functional language because everything gets done in `R` using functions, and because functions always behave consistently: a given set of inputs (arguments) always returns the same output.

## `R` uses *objects*
In an interactive `R` session, you can have different pieces of data, *objects*, and `R` has different kinds of these, most of which are data structures. 

1. `vector`s store a single data type.
2. `matrix` or `array`s are higher dimensional vectors.
3. `data.frame`s (and many similar things) are tables, each column is a vector, but the columns can be different data types.
4. `list`s are hierarchical data structures, where neither dimension nor data type need to be uniform.
5. `function`s aren't data, but they are objects

And many more...

Today we'll use `vector`s and `data.frame`s, and some nuanced cousins of the `data.frame`.

## Creating objects
You can make an object by *assigning* (`<-`) to its name, like this:
```{r}
a <- 2
```
Now we have a scalar, a vector with one element. This element consists in the integer value 2. The new object is called `a`. Not `A`! **`R` is case sensitive**. You can see `a` pop up in the environment tab, with some extra meta information. `a` can be used like so:
```{r}
a+1
a^2
```
You can also remove `a` like so:
```{r}
rm(a)
```
Assignment also works using `=`, so whether to use `<-` or not is mostly an aesthetic choice. `=` is used to specify arguments, so perhaps to avoid confusion you'd prefer `=` for everything, but I personally like to keep concepts separate where possible.

This way of object creation is general, for all types and sizes of objects. You will see it used everywhere you look in `R`.

## More on functions
You can also assign the output straight from a function. For example:
```{r}
nums <- rnorm(n = 10, mean = 0, sd = 1)
nums
```
Here we have created a new object `nums` by assigning the output of the function `rnorm()` (ten random draws from a standard normal distribution). See how we use `rnorm()` there? This function has three *arguments* (i.e. parameters), in this case intuitively named if you've ever taken an intro statistics class. Now the object `nums` is simply available to use throughout the remainder of this `R` session, yay. 

## Packages extend R

Packages are self-contained and documented collections of functions

Let's get a `data.frame` to play with: Hans Rosling's famous data, which you can get as follows: First we download and install an `R` package (library) called `gapminder` using the function `install.packages()`. You only need to do this once to get the package.

```{r, eval = FALSE}
install.packages("gapminder")
```
This one installs quick because it doesn't depend on other packages. Here's one we'll use that has lots of dependencies, so it'll take a while to download and install all its pieces.

```{r, eval = FALSE}
install.packages("tidyverse")
```

`R` has thousands of packages, built by developers and users alike, and they are tied together into an ecosystem. Many packages get written by researchers to make research easier, and there are several demographers out there making `R` packages. `R` has a big community of users and developers: let's find it in different places online...


*** Time-out to find the `R` community ***


Places to visit:
1. CRAN
2. StackOverflow
3. github

So some people created `gapminder` somewhere, uploaded it to `R`'s central repository, and then we installed it on our machines straight from an `R` session. Now you can load the package using the `library()` function. This loads the contents of the package into memory, i.e. makes them objects in your `R` session. The main object in this package is a `data.frame` called `gapminder`.

```{r}
library(gapminder)
```

## Reading help files
You can learn about a particular function in different ways. `R` functions are usually documented, and we can see the help file by typing `?` in front of the function name in the console:
```{r, eval = FALSE}
?rnorm
```
Do this live and see how the help page opens by default in the lower right pane in `Rstudio`. Such help pages are structured, and it's worth learning their basic parts so you can make good use of them. The **Description** section tells you the purpose, and is short and to the point. The **Usage** section tells you how a function *call* might look like if all arguments are used. For `rnorm()` we see `rnorm(n, mean = 0, sd = 1)`. From this we learn that `n` is a necessary argument, whereas `mean` and `sd` have default values of 0 and 1, and are therefore *optional* arguments. You can get more information about what's expected of the arguments in the **Arguments** section. The **Details** section gives further information on methods and other things you might want to be aware of. The **Value** section describes function output, and it might be structured. The **Examples** section is probably the most immediately useful, as it gives working examples that often gives hints on how to set things up to effectively use the function.

Even so, sometimes help files are terse. If the reason you're reading them is an error or warning, then you can often find answers in [Stack Overflow](https://stackoverflow.com/questions/tagged/r). You can also save yourself the trouble of copying and pasting error messages into a search engine by using the `searcher` package.

You only need to install it once.
```{r, eval = FALSE}
install.packages("searcher")
```

The following works in live `R` sessions, but not from an `Rmarkdown` chunk in my experience, FYI.
```{r, eval = FALSE}
library(searcher)
sum + mean # invoke an error
search_google() # by default searches this error
```

# Tidyverse basics
The above basics are incomplete, but applicable no matter what approach you take to `R` programming. Notably missing is an overview to subsetting and selecting, and this is because it depends on your approach. Now we're going to get started using some basic tools from the `tidyverse`, `ggplot` construction basics and `dplyr` data processing pipelines in particular so we'll see how a variety of operations work in the tidy approach. In later sessions we'll see some more traditional methods of filtering and selection.

## tidy data
*Tidy* data refers to tables of a particular layout, with one observation per row and one variable per column. Tidy datasets processed using tidyverse tools allow for fast and understandable analyses that in many cases require no programming, whereas it often takes a certain amount of head-scratching (programming) to analyze not-tidy datasets. 

### Example of tidy data
For example, you're probably all familiar with HMD lifetables [@HMD]: This data is tidy. If you were to take all the HMD single age lifetables and stack them, adding new grouping columns for Sex and Country, then this would still be tidy. That is, a single HMD observation consists in a unique combination of Country, Sex, Year, and Age. Each lifetable column is a variable, so the tidy way to store them is the traditional way: in columns. 

### Examples of not tidy data.
1. The DemoData database is not tidy: it's much longer than tidy data. All variables are stacked, and the variable type (qx, lx, counts, etc) is coded in an additional grouping column. This is efficient for storage, given that not every observation has every variable. It is not efficient for interactive programming because observations are potentially scattered over rows.

2. A matrix with ages in rows and years in columns can be very convenient for traditional programming, but it does not follow our definition of tidy. To make it tidy, we would need columns for age and year, and a new column containing the variable stored in the matrix.

3. Often panel data is delivered with each row representing an individual, and with many many variables. If there are 100 variables, and 10 waves, we end up with 1000 or so columns. For many analyses this is not considered tidy: an observation is a unique combination of individual and wave. To make it tidy, we'd need a column to store the wave variable.

Different data structures exist for various reasons, but in the end tidy data facilitates the kind of thinking and operations needed for a particular system of flexible data analysis and visualization.

## the gapminder `data.frame`
Let's work with a tidy `data.frame`. These are like a rectangular spreadsheet, or like a dataset in `Stata`: `data.frame`s have rows and columns, and there can be different kinds of data between columns, but only one kind of data within a column. Load it along with `tidyverse` in case you didn't before:

```{r}
library(gapminder)
library(tidyverse)
```

There are different ways to get orient yourself with a new data object.


*** Clicky demonstration of viewing data ***


Or you could get metadata about the object from `str()` (structure), or `glimpse()` from the tidyverse. This tells us that we have 1704 observations (rows) of 6 variables, and then it tells us the variable (column) names, what kind of data it is (`Factor` = categorical, `int` = integer, `num` = numeric), and a sample of the first few observations in each.

```{r}
str(gapminder)
```

## Basic `ggplot2` with the gapminder data
### Overview
Tidy datasets such as this can also be visualized without further ado using a systematic grammar [@wilkinson2012grammar] implemented in the `ggplot2` package ( @wickham2016ggplot2, this loads automatically with `tidyverse`). The `gapminder` examples I'll give today and tomorrow are either gratuitously lifted or modified from @healy2018data, which you can either purchase as the book (I'll show it to you), or refer to the free version online: [www.socviz.co](www.socviz.co). It's a fabulous book. Let's take a first look:

### *Map* to aesthetics
```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() 
```

The `ggplot()` function *maps* variables in the `gapminder` dataset to either coordinates (x,y) or aesthetics (for example color). This sets up the plot metadata, but it does not plot the data because we didn't tell it how to do so: this final step is done by adding a point geometry `geom_point()`. 

Notes: The first argument specifies the dataset. The `mapping` argument is specified always as `mapping = aes()`, where *aes* stands for aesthetics. At a minimum you'll want to map to `x`, but in this case also `y` and `color`. The things that you can map are variables in the tidy dataset. And where necessary `ggplot2` guesses whether they are quantitative or categorical in nature. You need to give a mapping to `ggplot()`, but you can also give different mappings for different geoms specified directly in the `geom` functions. That only makes sense if you'll have more than one `geom` going into the plot (we'll see this). Further modifications to the coordinate base created by `ggplot()` are *added* in to the expression with `+`.

### Adjust axis scales
```{r}
library(scales)
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() +
	scale_x_log10(labels = dollar)
```

It turns out that by specifying a nice large set of possible mappings, coordinate systems, geoms, and discrete and continous options for scaling mappings, that you can create just about any plot. For other tricky ones, there are usually packages available. Here's a nice overview of `ggplot2` functionality: [https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

### `geom`s and `facet`s
My objective now is to show you a few different geoms and ways of doing panels in `ggplot2`. 

For example, we can also add in a smoother to see the global trend:
```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
	geom_point(mapping = aes(color = continent)) +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar)
```

See how geoms can also have their own special mapping? Had we left color in the first mapping, then everything that follows would have been split on continent. See:

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar)
```
Wow, that's a noisy plot! What if instead of color we just split it into subplots by continent?

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
	geom_point() +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar) + 
	facet_wrap(~continent)
```
We get panel plots like this by specifying a layout formula in `facet_wrap()`, where `~` separates left and right sides of the fomula, and where left means rows in a panel layout and right means columns. Since there's nothing on the left it just orders the continents. Color is no longer needed since the groups are separated.

There is a time variable in this dataset that we've basically been ignoring. How about a `gdpPercap` time series?

Too busy?
```{r}
ggplot(gapminder, mapping = aes(x = year, y = gdpPercap, by = country)) +
	geom_line() 
```

Use facetting to seprate the continents
```{r}
ggplot(gapminder, mapping = aes(x = year, y = gdpPercap, by = country)) +
	geom_line() +
	facet_wrap(~continent)
```

### Summary lessons 
More sophisticated usage of `ggplot2` builds on these basics in a rather consistent way. You can deepen your fluency by learning new `geoms`, learning what things can be *mapped* to aesthetics, and learning options to scale axes and mappings. To get an overview, refer to the `ggplot2` cheat sheet. For a very approachable introduction, try the Healy book. For the most thorough and advanced treatment, refer to the Wickham book. To learn usage, look at help examples, or R graph gallery examples (add links). In following lessons I will sometimes use `ggplot2`, and I will slip in further tips in an unstructured way.

## Basic `dplyr`
`ggplot2` is a wonderful tool if your data is already both *tidy* and *mappable*. But source data rarely is. To feed a dataset to `ggplot2` often we'll want to preprocess data, or even model it. The tools for data management, shaping, matching, sorting, joining, filtering, and so forth (stuff to prep for `ggplot2`) blend seamlessly into the tools of data analysis and modeling when one uses the `dplyr` approach. Some basic concepts will help us get started:

1. `dplyr` functions are *verbal* because they *do* things to data.
2. A bunch of `dplyr` functions executed in sequence on a dataset for a *pipeline*.
3. Data processing pipelines read as sentences, because verbs are strung together using pipes. This is the pipe symbol: `%>%`
4. Data processing pipelines are quick to build, easy to read, and help keep your `R` workspace less cluttered than most alternatives. Clean coding and clean workspaces means code is easier to maintain, even when not written by yourself!

This probably sounds jargony, so let's hop to it:

### Verbs {.tabset}
#### Overview
Here are a selection of `dplyr` verbs. Some things to note: the first argument of each is a data object, usually something like a `data.frame`. These do things to the dataset, and they return the dataset back.

1. `group_by()` allows operations on subgroups
2. `gather()` make a wide range of columns tidy
3. `mutate()` make new columns using other columns, no loss of rows
4. `summarize()` aggregates over rows. Usually reduces nr of rows
5. `select()` selects columns
6. `filter()` selects rows (subsets)
7. `ungroup()` removes the former

We'll see other `dplyr` verbs here and there during this workshop.

Flip through the tabs in order.

#### `group_by()` 
When you want to operate on groups in a dataset, then it is necessary to declare the grouping.
```{r}
gap_grouped    <- group_by(gapminder, country, year, continent) 
head(gap_grouped)
```
Grouping does not do anything to the data directly. All values will be identical, and the dimensions are the same. It just adds a piece of metadata to the object that declares the grouping.

NB: the object created here is used in the following code chunk.

#### `gather()`
Here we use `gather()` to pull all the gapminder value columns into a single key-value pair, where the key is the variable column name, and value is the contents. The resulting `data.frame` is three times longer because there are three variable columns (and three key columns), and this mimicks the setup of your own `DemoData` SQL database. 
```{r, message = FALSE, warning = FALSE}
gapminder_long <- gather(gap_grouped, 
						 key="varname", 
						 value = "value", 
						 lifeExp:gdpPercap) 
```
Note: `group_by()` is a necessary prestep in order to maintain the unique key combinations. We could consider `continent` either a key or a variable. It's not strictly needed as a key because country-year uniquely identifies observations, but I left it there to not over-lengthen the dataset.

You can get back to the original format with the `spread()`, which is the opposite of `gather()`.
```{r, eval = FALSE}
spread(gapminder_long, varname, value)
```

#### `mutate()` 
`mutate()` adds columns to, or modifies columns in, a `data.frame`. Think of the example of lifetable construction: You might start with deaths and exposures, and all other columns are created based on these. This is a case for `mutate()`, and we'll do this sort of exercise when we get to function programming. The main thing to remember is that `mutate()` does not change the number of rows. Keeping with our gapminder example, say we want to add a column for the mean life expectancy, but repeated for each row within each group. To get these means for each country, we'll want to group by country first (our previous grouping also grouped by year).
```{r}
gap_grouped <- group_by(gapminder, country)
gap_mutated <- mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp)) 
head(gap_mutated)
```
You can also just keep comma separating newly made columns, like so:
```{r, eval = FALSE}
mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp),
	   sd_lifeExp = sd(lifeExp),
	   cov_lifeExp = sd_lifeExp / mean_lifeExp,
	   cov_pop = sd(population) / mean(population)) 
```
You see how `cov_lifeExp` is created using columns being created within the same call? Pretty neat! Expressions can also be more complicated, see where `cov_pop` is created using two function calls.

NB: The object `gap_grouped` created here is also used in the next code chunk.
#### `summarize()`
If instead we wished to collapse the dataset to have one observation per country, then we use `summarize()` (this can also by lossy with respect to columns). Here is that first `mutate` expression when executed with `summarize()`
```{r}
gap_summarize <- summarize(gap_grouped,
	   mean_lifeExp = mean(lifeExp))
head(gap_summarize)
dim(gap_summarize) # collapsed
```
If you want to summarize more than one column, then this works, also with sequential dependency over columns created on the fly:
```{r}
gap_summarize <- summarize(gap_grouped,
	   mean_lifeExp = mean(lifeExp),
	   mean_gdpPercap = mean(gdpPercap),
	   sd_gdpPercap = sd(gdpPercap),
	   cov_gdpPercap = sd_gdpPercap / mean_gdpPercap)
head(gap_summarize)
```
You can get a lot of mileage out of `group_by()`, `mutate()` and `summarize()`.
#### `select()` 
`select()` pikcs out columns. 
```{r}
head(select(gapminder, country, gdpPercap))
```
If data are grouped, the key columns are retained (and it tells you so in the console):
```{r}
gap_grouped <- group_by(gapminder, country, year)
head(select(gap_grouped, gdpPercap))
```
#### `filter()` 
More often we want to subset rows to those that satisfy certain selection criteria. Use `filter()` for this.
```{r}
gap_filtered <- filter(gapminder,
					   year >= 2000,
					   year <= 2010,
					   continent == "Africa")
str(gap_filtered)
```
There are three logical expressions here, each producing a value of TRUE or FALSE for each row. Comma separation between these is interpreted as `&`, so the full logical expression evaluated is interpreted as: 
`year >= 2000 & year <= 2010 & continent == "Africa"`
Note that double equals `==` is for the logical evaluation of equality! The other logical operators are pretty straightforward. A vertical bar `|` is used for *or*.

#### `ungroup()` 
It is often a good idea to remove a grouping attribute from a data object when you're done using it. Otherwise you might operate within groups when you don't mean to. Then execute `ungroup()` on the object. This is also a clean way to remove a grouping before declaring a new and different one. For example, say we want to make a column for the mean `lifeExp` within the series for each country, and also the mean over countries within each year. Then we'll need to switch groupings:
```{r}
gap_gr1 <- group_by(gapminder, country)
gap_gr1 <- mutate(gap_gr1,
				  mean_lifeExp_country = mean(lifeExp))
# now undo to operate on a different group
gap_gr2 <- ungroup(gap_gr1)
gap_gr2 <- group_by(gap_gr2, year)
gap_gr2 <- mutate(gap_gr2, mean_lifeExp_year = mean(lifeExp))
head(gap_gr2)
```

### Pipes
I had to force myself not to use pipes in the previous section! Really `dplyr` functions can be executed in sequence without the need to create intermediate objects. If you executed the previous examples, look at how cluttered RStudio's environment tab has become. We can avoid all that by stringing things together using the pipe operator from the `magrittr` package that loads automatically with `tidyverse`. here are some of those example reworked. See how they read as sentences?

Before:
```{r, eval = FALSE}
gap_grouped <- group_by(gapminder, country)
gap_mutated <- mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp)) 
head(gap_mutated)
```
After:
```{r, eval = FALSE}
gapminder %>% 
	group_by(country) %>% 
	mutate(mean_lifeExp = mean(lifeExp)) %>% 
	head()
```
Note: the first argument (`data`) is automatically filled with the object running through the pipeline. You should imagine `gapminder` running through a staged pipeline, getting modified at each step. When it gets to the end we just show the first several rows in the console.

Before:
```{r, eval = FALSE}
gap_gr1 <- group_by(gapminder, country)
gap_gr1 <- mutate(gap_gr1,
				  mean_lifeExp_country = mean(lifeExp))
# now undo to operate on a different group
gap_gr2 <- ungroup(gap_gr1)
gap_gr2 <- group_by(gap_gr2, year)
gap_gr2 <- mutate(gap_gr2, mean_lifeExp_year = mean(lifeExp))
head(gap_gr2)
```
After:
```{r, eval = FALSE}
gapminder %>% 
	group_by(country) %>% 
    mutate(mean_lifeExp_country = mean(lifeExp)) %>% 
	ungroup() %>% 
    group_by(year) %>% 
    mutate(mean_lifeExp_year = mean(lifeExp)) %>% 
	head()
```
Instead of ending in `head()`, we could assign the result to a new object. This is usually done at the start, like so:
```{r, eval = FALSE}
gap_mutated <- 
	gapminder %>% 
	group_by(country) %>% 
    mutate(mean_lifeExp_country = mean(lifeExp)) %>% 
	ungroup() %>% 
    group_by(year) %>% 
    mutate(mean_lifeExp_year = mean(lifeExp)) 
```
The object `gap_mutated` consists in the fully-executed pipeline. 

Note: We usually construct pipelines like these incrementally, often checking as we to make sure things work as expected. Often that checking is done like we saw above, with ` %>% head()` or similar as the last step. As we determine next steps, this checker line gets pushed further down in the chain of execution. Just remember to remove it when you're done building the pipeline!

Note also: We use indentation and alignment to make the code more legible.

As mentioned, `dplyr` pipelines also read as sentences. This last one can be verbalized as "first take the gapminder dataset, then group it by country, then make a new column for the average life expectancy over time within the country, then regroup by year and make a new column for the average life expectancy over countries within the year". Such sentences don't make for good lit but they do make your code accessible to readers, which is a major advantage. You can (should) also comment as you go.

Legibility of a script is acheived by reducing clutter and by using succinct pipelines. This is important because your processing/analysis code becomes all the more shareable, both with others and with future you. Who hasn't looked back at your own code written years ago and wonders wha the heck it does? It feels awful to have to reverse engineer your own logic and coding. Yes, it can be overcome with good annotations, but let's obviate the need altogether if we can.

As mentioned, I'll slip in pipelines and `ggplot2` here and there in the coming days, so there will be several opportunities for practice and repetition.

# Reading data into `R`
This is all good and well if I happen to give you a dataset to play with, but I'm sure you're itching to have a go at your own data. Problem: you probably have different datasets in different formats, probably because at times different tools are used to do the business of demography. Don't worry, you're covered. There are oodles of ways to read data into `R`. In keeping with the above approach to `R`, let's introduce two packages from the tidyverse. `readr` and `haven`. `readr` gives user friendly ways to read in datasets in generic formats, whereas `haven` helps read in several proprietary formats. If a given data format isn't already accounted for in either of these packages, there's very likely at least one other `R` package out there that does the job. Let's just plow through several examples.

## Different data formats
Here are a selection of functions that read in selected data formats. I've resaved the `gapminder` data in several formats in the `data` folder.
### csv
`read_csv()`, `read_tsv()`, `read_delim()` will read in data delimited by commas, tabs, or any character, respectively. These have nice default options. They all work similarly.
```{r, eval = FALSE, echo = FALSE}
library(here)
write_csv(gapminder, path = here("data","gapminder.csv"))
write_delim(gapminder, path = here("data","gapminder_tab.csv"), delim = "\t")
write_delim(gapminder, path = here("data","gapminder_hash.csv"), delim = "#")
```
```{r}
library(here)
gapminder <- read_csv(here("data","gapminder.csv"))
gapminder <- read_tsv(here("data","gapminder_tab.csv"))
gapminder <- read_delim(here("data","gapminder_hash.csv"), delim = "#")
```
This would be the same if the files didn't have `.csv` ending. The file name doesn't matter as long as it just contains text. So `.txt` is OK, and other spellings.
### SPSS, SAS, STATA
Proprietary software systems tend to redesign their native file formats periodically, but do not always openly publish metadata on how to interpret them. The `haven` package takes care of the *big three*. It is installed with `tidyverse`, but isn't automatically loaded with it, so you need to do `library(haven)` to use it. Labels are handled nicely, but our example doesn't use this feature.

STATA is supported for versions 8-15 at present. If you use these systems, then the file extensions will look familiar to you.
```{r, eval = FALSE, echo = FALSE}
library(haven)
write_dta(gapminder, path = here("data","gapminder.dta"), version = 14)
write_sav(gapminder, path = here("data","gapminder.sav"))
write_sas(gapminder,path = here("data","gapminder.sas7bdat"))
```
```{r}
library(haven)
gapminder <- read_dta(here("data","gapminder.dta"))
gapminder <- read_sav(here("data","gapminder.sav"))
gapminder <- read_sas(here("data","gapminder.sas7bdat"))
```
### spreadsheets
Spreadheets are sort of a headache, and there are many competing methods in `R` to grab data from particular cell ranges and tabs. These are often OS dependent, so it's hard to recommend a single method for everyone.
### flat files
`read_fwf()` comes with the `readr` package, so it's loaded already with `tidyverse`. To read in a file with fixed column positions (flat files), you always need to specify column data. `readr` makes it a little bit less tedious by detecting column positions, assuming that there's at least one space between columns. Otherwise (like with US birth or death microdata) you'll still need to specify start and end positions. It also detects column classes, but to be on the safe side you can specify them yourselves using a nice string shorthand (`c` is character and `d` is double).
```{r, results = "hide", echo = FALSE, message = FALSE}
library(gdata)
write.fwf(as.data.frame(gapminder), 
 		  file = here("data","gapminder_flat.txt"),
		  colnames =FALSE)
```
```{r}
# detect column properties, and declare header:
fwf_metadata <- fwf_empty(here("data","gapminder_flat.txt"),   
		                  col_names = c("country","continent","year",
		 				  "lifeExp","pop","gdpPercap"))
gapminder   <- read_fwf(here("data","gapminder_flat.txt"),
		 col_positions = fwf_metadata,
		 col_types = "ccdddd")
```
In this case we had nice white space padding, so everything could be detected, but if your data are super packed together, like with US birth and death microdata, then you'll need to build up a similar object by hand. You stick the information in a self-made list like this:
```{r}
fwf_metadata <- list(
	begin = c(0,25,34,39,50,60),
	end = c(11,33,38,47,58,NA),
	skip = 0, # how many lines should we skip?
	colnames = c("country", "continent", "year", 
				 "lifeExp", "pop", "gdpPercap"))
```


```{r}

```
`fread()` from the `data.table` is also great.
### SQL databases
`sqldf()` from the `sqldf` package is good with SQL databases of all shapes and sizes. It also lets you do extractions using native SQL syntax, in case that's a familiar way of working for you. The `DBI` package has a nice explainer for interfacing with databases, and it's vignette is a good tutorial you can get the vignette by doing this
```{r, eval = FALSE}
install.packages("DBI")
library(DBI)
# this will build the vignette and open it in the right panel.
vignette("spec", package = "DBI")
```

### Native `R` files
`R` has more than one native compression type, I suggest you use `.rds` if you want to save an `R` object that you know will be kept in `R`. This has the advantage of fast and unambiguous loading, since no parsing is required and since loading happens via assignment (as with other read functions). Writing and reading look like this:
```{r, eval = FALSE}
saveRDS(gapminder,file = "gapminder.rds")
gapminder <- readRDS("gapminder.rds")
```




### Requests?
If you come across some other kind of data, take a peek at the [Data Import Cheat Sheet](https://resources.rstudio.com/rstudio-cheatsheets/data-import-cheat-sheet)  need help figuring out how to get things humming using these or other data formats let me know. It's quite possible that these situations are shared, so if I figure it out then a lesson can be made out of it. 







# Exercises

1. Try making the gapminder scatterplot with different smoothing methods: i) on the whole dataset and ii) by continent. You can find out the other methods by reading the help file, typing `?geom_smooth`

2. Here's a way to remove some rows from a dataset:
```{r}
gaps <- filter(gapminder, continent != "Oceania")
```
Try subsetting `gapminder` to just the first and last years (and also removing Oceania). Plot the density of log `gdpPercap` by continents (tip: `fill = continent`). They will overlap (tip: try `alpha = .5` to make them transparent). Make a 2-panel plot showing how these distributions changed between 1952 and 2007.

3. TBD I'll make stuff up for exercises as we go

# References 


