---
pagetitle: "Day 4"
title: | 
  | Introduction to R for Demographers
  | \vspace{1.5cm} \LARGE\emph{Workshop outline}
author: |
  | United Nations Population Division
  | 19-23 August, 2019
  | Tim Riffe
  | Max-Planck-Institute for Demographic Research
date: "22 August, 2019"
output:
  html_document:
    number_sections: yes
    toc: yes
params:
  output_dir: "../RforUNPD2019/docs"
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center}\includegraphics[trim=0 0 0 8cm, width=6cm, ]{assets/MPIDR_square_color.pdf}\\[\bigskipamount]}
- \posttitle{\end{center}}
bibliography: bibliography.bib
---

<a href="https://github.com/timriffe/RforUNPD2019" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# reading in data from Excel
The `readxl` package installs automatically with `tidyverse`, but you need to load it directly to use it (we'll also load a couple other helpful packages):

```{r, message = FALSE}
library(tidyverse)
library(here)
library(readxl)
```

This packge offers read functionality, and it's also platform independent, and you don't actually need Excel installed to use it. I downloaded an example dataset from Eurostat. You can download it from the repository. Open the spreadsheet so you can see you it's formatted: this one has some common formatting aspects that make it tricky to machine read, so hopefully it simulates what you might actually find.

Let's grab the table from `Sheet 1`
```{r, message = FALSE}
EURODTHS <-
        read_xlsx(here("data","eurostat.xlsx"), 
                  sheet = "Sheet 1", # sheets are straightforward,
                                     # can also index by number
                  range = "A8:X63",  # this was eyeballed
          na = ":") %>%              # each stat agency seems to prefer
                                     # different NA codes.. best to specify
  # select cols using names (character, to be sure)
  select(c("TIME", as.character(2007:2018))) %>% 
  # TIME refered to the column headers,
  # whereas GEO refers to the row names,
  # which for us are a key variable.
  rename(GEO = TIME) %>% 
  # remove the empty first row with slice()
  slice(-1) %>% 
  # finally let's get it tidy (stack the Years, which are now in columns)
  gather(key = "Year", value = "Deaths", -GEO) 
```


# A more involved Excel example
That was rather satisfying wasn't it? How about something a little trickier? This one was raised by Aisha, and we worked half way through it on Wednesday. The rest of this was filled in live in session, and now I've gone through and annotated.

## Download two spreadsheets
Download from these two links and put the files straight in the `data` folder. (direct downloads):

1. TFR data:
[https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/2_Fertility/WPP2019_FERT_F04_TOTAL_FERTILITY.xlsx](https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/2_Fertility/WPP2019_FERT_F04_TOTAL_FERTILITY.xlsx)

2. Contraceptive use data:
[https://www.un.org/en/development/desa/population/publications/dataset/contraception/wcu2019/UNPD_WCU2019_Country_Data_Survey-Based.xlsx](https://www.un.org/en/development/desa/population/publications/dataset/contraception/wcu2019/UNPD_WCU2019_Country_Data_Survey-Based.xlsx)

(Yes, there are ways to do this programmatically from `R`)

## read in TFR dataset
```{r, message = FALSE, warning=FALSE}
path <- here("data", "WPP2019_FERT_F04_TOTAL_FERTILITY.xlsx")
TFR <- read_xlsx(path,
          sheet = "ESTIMATES",
          skip = 16,
          col_types = c(
            rep("text", 7),
            rep("numeric",14)),
          na = "â€¦") %>% 
  gather(key = "Year",
         value = "TFR",
         8:21) %>% 
  mutate(Year = substr(Year, 1, 4)) %>% 
  rename("ISO" = "Country code") %>% 
  filter(Year == "1990")
```
## read in family planning dataset
```{r}
path <- here("data", 
"Table_Model-based_estimates_Countries_2019.xlsx")
FP <- read_xlsx(path,
          sheet = "FP Indicators",
          skip = 4,
          col_types = c(
            rep("text",6),
            "numeric","numeric","text"
          ),
          na = "..") %>% 
  rename("MarStat" = "Marital status",
         "ISO" = "ISO code",
         "Med_UI" = "Median estimate and uncertainty intervals") %>% 
  filter(Indicator == "CPmod",
         Med_UI == "Median",
         MarStat == "AW",
         Year == "1990") 
```
## Join the datasets
Now we have two joinable datasets as `data.frame`s, so let's see how to do that using `dplyr`.
```{r, message = FALSE, warnings = FALSE}
FP_TFR <- left_join(FP,
                    TFR,
                    by = "ISO")
```
## Join SDG codes
```{r}
# get ISO-SDG lookup
ISOSDG <- read_xlsx(here("data","WPP2019_F01_LOCATIONS.xlsx"),
          sheet = "Sheet2",
          col_types = c("text","text"))

FP_TFR <- left_join(FP_TFR,
                    ISOSDG,
                    by = "ISO")
```
## Get SDG colors and labels
Now we make some helper objects for constructing the desired plot
```{r}
Colors <- c("#DEA0FD","#FF7F0E","#2ED9FF",
"#3283FE","#FFEE33","#AAF400",
"#1CBE4F","#B33E52")

names(Colors) <- c(947,1833,921,1832,1830,927,1835,1829)

labs <- c("Sub-Saharan Africa",
"Northern Africa and Western Asia",
"Central and Southern Asia",
"Eastern and South-Eastern Asia",
"Latin America and the Caribbean",
"Australia/New Zealand",
"Oceania*",
"Europe and Northern America")
names(labs) <- c(947,1833,921,1832,1830,927,1835,1829)
```
## Make the plot
Now for the plot:
```{r}
FP_TFR %>% 
  ggplot(mapping = aes(x = Percentage,
                       y = TFR,
                       color = SDG)) + 
  geom_point() + 
  scale_color_manual(labels = labs, 
                     values  = Colors) +
labs(x="Percent using\nmodern contraception",
 title ="Contraceptive prevalence versus\nTotal fertility rate\n185 countries, 1990, All women"
) + 
  theme_bw()
```

## Hypothetical function using `ggplot2` in it:
The trick is to be allowed to use strings as arguments to `aes()` and similar. For this there is a helper function, `aes_string()`.
```{r}
SDGscatter <- function(.data, 
                       x = "Percentage", 
                       y = "TFR"){
  # 1) get standard colors:
  Colors <- c("#DEA0FD","#FF7F0E","#2ED9FF",
"#3283FE","#FFEE33","#AAF400",
"#1CBE4F","#B33E52")
codes <-  c(947,1833,921,1832,1830,927,1835,1829)
names(Colors) <- codes

labs <- c("Sub-Saharan Africa",
"Northern Africa and Western Asia",
"Central and Southern Asia",
"Eastern and South-Eastern Asia",
"Latin America and the Caribbean",
"Australia/New Zealand",
"Oceania*",
"Europe and Northern America")
names(labs) <- codes
.data %>% 
  ggplot(mapping = aes_string(x = x,
                       y = y,
                       color = "SDG")) + 
  geom_point() + 
  scale_color_manual(labels = labs, 
                     values  = Colors) +
  theme_bw()
# labs(x="Percent using\nmodern contraception",
#  title ="Contraceptive prevalence versus\nTotal fertility rate\n185 countries, 1990, All women"
# )
}
# See it works:
SDGscatter(FP_TFR)
```

For this one would just want to dynamically `paste()` to arrive at reasonable titles based on the variables selected for x and y.


## `Tidycells`
Guess what everyone, you're lucky because a new package was released on Monday, `tidycells`. Literally this Monday, which supposedly handles the exact problem of reading strangely formatted spreadsheets and wrangling them into something sort-of tidy. Wait until it matures though, it seems. It didn't automatically handle the dataset above it seems.

## `clean`
Guess what everyone, you're lucky because a new package was released on Tuesday, `clean`. Literally this Tuesday, which gives tools for data cleaning. Let's take a look:

```{r, eval = FALSE}
install.packages("clean")
```

## numeric examples
Sometimes if you read in data from an Excel spreadsheet, the number formatting is retained, such that the column is read as `character`, even though it's supposed to be representing a number. Coercion to numeric will result in `NA`. Compare:

```{r}
# one hundred thousand. 
as.numeric("100.000") # wrong!
as.numeric("100000")  # ok
as.numeric("100,000") # doesn't know what to do, NA
```
Issues such as this can be dealt with using string operations, but even in the best of circumstances this is tedious, and that's where the clean package can help. Here are some situations:
```{r}
library(clean)
# let's increase how many decimals print to the screen for this:
options(digits=9)
# space-separated thousands!
clean_numeric(c("100 000", "234 111"))
clean_numeric(c("100   000", "234   111"))

# an absurd case
clean_numeric(c("100 000bd.01", "234 zz111,02")) 

# more common cases
clean_numeric(c("100,000", "234,111"), remove = "\\.")
clean_numeric(c("100.000", "234.111"), remove = "\\.")

# remove commas, keep decimals
clean_numeric(c("100,000.01", "234,111.02"), remove = "\\,")
# remove dot thousand sep, keep comma as decimal
clean_numeric(c("100.000,01", "234.111,02"), remove = "\\\\.")
```
For other cases, the `remove` argument needs to be modified. It used a character pattern system called `regex` (regular expressions), which is terse, but almost universal. That means you can google almost any situation and find the regex to handle it. Or you can try interactive tools to get the right regex, for example: [https://regexr.com/](https://regexr.com/)

## logical examples
There are also cleaning helpers for other kinds of data. Another nice one is for coverting things to logical:
```{r}
responses <- c("no","yes","unknown","unk","Yes","No way")
clean_logical(responses)
```
It's aware of a bunch of languages, and you can also provide a lexicon of what counts and true and false using arguments. To learn more, explore the package help. Presumably the authors will release a vignette or webpage soon.

# Using `DemoTools`
We've looked a little bit at code from `DemoTools` to see examples for function-writing, and for use of `if` statements for conditional code execution. But we've not spoken about what's in that package. It's one of many packages that contain useful things for demographers.

## Installation
This package is only on github for the time being, so to install it you'll need to use `devtools`. If there are problems for Windows users we'll have to solve them on the fly.

```{r, eval = FALSE}
# install if needed:
# install.packages("devtools")
library(devtools)
install_github("timriffe/DemoTools")
```

And now you load it as normally:
```{r, message = FALSE}
library(here)
library(tidyverse)
library(DemoTools)
```

## What it has

There are lots of old-school deterministic demographic methods in this package. At present they include operations for detecting potential problems in age structure and correcting for (some of) them. There is also fairly developed support for calculating abridged lifetables (standard 5-year age groups), including extrapolative closeouts, and assorted other things. The package also comes with some data, and examples in the help files show how to use the functions. There are also a couple tutorials: one on smoothing, and the other on age-heaping detection.

Stuff to be added this year (major additions):

1. Single age lifetable functionality
2. Intercensal interpolation methods
3. Model lifetable functions
4. More tutorials, a web front, and maybe a cheat sheet

## Age heaping indices
We have a bunch of heaping indices, in case one or more are familiar to you. The example dataset is heavily heaped on the 0s and 5s, see:
```{r}
data.frame(Age=0:99, Pop = pop1m_pasex) %>% 
  ggplot(mapping = aes(x = Age, y = Pop)) +
  geom_line()
```
Just how bad? All of the indices will tell us it's bad, but they're on different scales, so not strictly comparable without some sort of transformation (which no one has bothered to think about). Anyway, here's Myers method, which is an overall judgement of an age range. The result is on a percent scale.
```{r}
Myers(pop1m_pasex, Age = 0:99)
```
Whipple is probably also familiar, and it lets you test one digit at a time. Except 0 and 5 can be tested together. These are on a ratio scale.
```{r}
Whipple(pop1m_pasex, Age = 0:99, digit = 0)
Whipple(pop1m_pasex, Age = 0:99, digit = 5)
Whipple(pop1m_pasex, Age = 0:99, digit = c(0,5))
Whipple(pop1m_pasex, Age = 0:99, digit = 1)
```
Other methods include `Bachi()`, `Spoorenberg()` (Yes, your Spoorenberg!) and `Noumbissi()`. `KannistoHeap()` tests for heaping in old ages, for a single age at a time. `Jdanov()` is also tailored to old ages ending in 0 or 5. 

In case you want to get a feel for the scale that one or more of these indices is working on, there is a utility called `heapify()` that *induces* heaping. 

Finally, the function `zero_pref_sawtooth()` asks the particular question "is heaping worse on 0s than on 5s? So bad that it would make a zig-zag pattern even in 5-year grouped data?", while `five_year_roughness()` simiarly measures the overall roughness of the age distribution were it to be grouped to 5-year age groups. These last two quality indices are unique to this package and not in the literature. Their purpose is to help decide whether more aggressive smoothing over 5-year age groups is desirable before coming back to single ages.

Here's an example of roughness.
```{r}
Scenarios <- data.frame(Age = 0:99,
            orig = pop1m_pasex,
            SM = sprague(agesmth(pop1m_pasex, 
				             Age = 0:99, 
				             method = "Strong", 
			               OAG = FALSE, 
				             young.tail = "Arriaga"),
		             OAG = FALSE)) %>% 
  # perturb the structure:
   mutate(h1 = heapify(SM, Age, p0 = 1, p5 = 1),
          h2 = heapify(SM, Age, p0 = 2, p5 = 2),
          h3 = heapify(SM, Age, p0 = 2.5, p5 = 2)) %>% 
  # make it tidy:
   gather(key = "scenario",
          value = "Pop",
          orig:h3) %>% 
   arrange(scenario, Age)
```

Take a look, single ages. These are extreme.
```{r}
Scenarios %>% 
   ggplot(mapping = aes(x = Age, y = Pop, color = scenario)) +
   geom_line() + 
   xlim(20,80) + 
   ylim(0,2e5)
```

How do they look when grouped to 5-years though?
```{r}
Scenarios %>% 
   group_by(scenario) %>% 
   mutate(A5 = calcAgeAbr(Age)) %>% 
   ungroup() %>% 
   group_by(scenario, A5) %>% 
   summarize(P5 = sum(Pop)) %>% 
   ungroup() %>% 
   ggplot(mapping = aes(x = A5, y = P5, color = scenario)) +
   geom_line() +
   xlim(20,80) + ylim(0,2e5)
```


```{r}
library(knitr)
Scenarios %>% 
  group_by(scenario) %>% 
  summarize(rough = five_year_roughness(Pop, Age),
            sawtooth = zero_pref_sawtooth(Pop, Age),
            Whipple = Whipple(Pop, Age, digit = c(0,5)),
            Myer = Myers(Pop, Age)) %>% 
  kable(digits = 2)
```

Let's discuss this table. First, Myers and Whipple basically agree. But there are patterns that they don't detect too. h2, h3, and the original data would be rough-looking, even if we grouped to 5-year ages! Further, especially h3 would also show a sawtooth pattern, with five-year age groups on the 0s preferred over 5s. This last scenario recommends first treating with on of the `agesmth()` methods, described in the next section, before then going back to single ages.

There are other functions one could use to similar effect:
`beers()`, `groupAges()` with `splitMono()`, `splitUniform()` (a utility)

An example of group + constrained split:
```{r}
P5 <- groupAges(pop1m_pasex, Age=0:99)
P1 <- splitMono(P5, Age = seq(0,95,by=5))
```

## age smoothing
But what about for cases when even grouping to 5-year ages isn't enough?
The function `agesmth()` contains several methods for this case.

```{r}
p1 <- agesmth(P5, 
        Age = seq(0,95,by=5),
        method = "Carrier-Farrag")
data.frame(Age = seq(0,95,by=5),
           P5 =P5,
           p1 = p1) %>% 
  gather(key ="scenario",
         value = "Pop",
         2:3) %>% 
  ggplot(mapping = aes(x = Age,
                       y= Pop,
                       color = scenario)) + 
  geom_line()
```

## setting up a methods test
You could do more systematic testing of smoothing methods this way. Here, Karup-King-Newton is applied, but you could add more methods in the `mutate()~ call at the end. This is fixed now. The line `A5 = Age - Age %% 5` was previosuly `calcAgeAbr()`, which groupds infants and children separately.
```{r}
KKN_test <- 
  Scenarios %>% 
  group_by(scenario) %>% 
  mutate(A5 = Age - Age %% 5) %>% 
  ungroup() %>% 
  group_by(scenario, A5) %>% 
  summarize(P5 = sum(Pop)) %>% 
  ungroup() %>% 
  group_by(scenario) %>% 
  mutate(KKN = agesmth(P5, Age = A5, method = "kkn")) 
```

## abridged lifetable function
This was taken from the help file for `LTabr()`. It reproduces a PAS spreadsheet. There are many other options to control how it gets done.
```{r}
#?LTabr
Exposures <- c(100958,466275,624134,559559,446736,370653,301862,249409,247473,223014,172260,149338,127242,105715,79614,53660,
		31021,16805,8000,4000,2000,1000)
Deaths <- c(8674,1592,618,411,755,1098,1100,1357,1335,3257,2200,4023,2167,4578,2956,4212,2887,2351,1500,900,500,300)

Age    <- c(0, 1, seq(5, 100, by = 5))
AgeInt <- c(diff(Age), NA)

PASLT <- LTabr(
    Deaths = Deaths, 
		Exposures = Exposures, 
		Age = Age,
		AgeInt = AgeInt,
		axmethod = "pas",
		IMR = .1,
		region = "n",
		Sex = "m")
```
## using demotools in custom utility functions
You can make a helper function to calculate the lifetable but return only one column (more in keeping with the lightweight vector-output preference in the package)
```{r}
LTcolumn <- function(column = "ex", ...){
  LTabr(...)[,column]
}
```
# Reporting `DemoTools` bugs
We give it a whirl, trying out some of the extrapolation methods. Interestingly this one gives unexpected output:`NaN` in the open age group for $a(x)$ and $e(x)$.
```{r}
# This seems to have induced a bug, so we saw how to file a usable bug report.
test <- LTabr(
    Deaths = Deaths[1:17], 
		Exposures = Exposures[1:17], 
		Age = Age[1:17],
		AgeInt = AgeInt[1:17],
		OAG = FALSE,
		OAnew = 110,
		extrapLaw = "gompertz",
		extrapFrom = 75,
		extrapFit = seq(40, 75, by = 5),
	)
tail(test)
```
We then filed the following potential bug report:
[https://github.com/timriffe/DemoTools/issues/82](https://github.com/timriffe/DemoTools/issues/82)



# References 
